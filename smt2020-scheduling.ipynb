{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"13907d51","cell_type":"code","source":"# !rm -r /kaggle/working\n# !git clone https://github.com/ce-muzzamil/semiconductor_fabrication_scheduling.git\n# !cp -v -r /kaggle/working/semiconductor_fabrication_scheduling/* /kaggle/working/\n# !rm -r /kaggle/working/semiconductor_fabrication_scheduling\n\nimport sys\nsys.path.append(\"/kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f3e7ba4a-516c-423a-8c0b-a3abe0b04158","cell_type":"markdown","source":"# Imports","metadata":{}},{"id":"bbd20a75","cell_type":"code","source":"import os\nimport time\nimport numpy as np\nfrom simulation.file_instance import FileInstance\nfrom simulation.read import read_all\nfrom simulation.dispatching.dispatcher import dispatcher_map\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom torch.optim import Adam\n\nfrom logger import Logger\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca28866f-f0c3-460d-a90d-36ce0e94d593","cell_type":"markdown","source":"# Environment for the dispatching agent (Inspired from the original env in the Repo)","metadata":{}},{"id":"e4f8c599","cell_type":"code","source":"class SCFabEnv:\n    def __init__(self, dataset, days=1, dispatcher='fifo', seed=0):\n        self.files = read_all('datasets/' + dataset)\n        self.instance = None\n        self.days = days\n        self.seed_val = seed\n        self.dispatcher = dispatcher_map[dispatcher]\n\n        self.none2zero = lambda x: 0.0 if x is None or x == '' else x \n\n    def reset(self, hard=True):\n        if not hard:\n            self.throughput = 0\n            self.tardiness = 0\n            return\n\n        self.completed_on_time = 0\n        \n        self.throughput = 0\n        self.tardiness = 0\n        run_to = 3600 * 24 * self.days\n        self.eid = np.random.randint(999_999_999)\n        self.instance = FileInstance(self.files, run_to, True, [])\n        self.num_lots_done = 0\n        self.lots_done = []\n        self.lots_dispatched = []\n        self.groups = {}\n        for machine in self.instance.machines:\n            if machine.group not in self.groups:\n                self.groups[machine.group] = set()\n            self.groups[machine.group].add(machine.family)\n\n        self.families = {}\n        for machine in self.instance.machines:\n            if machine.family not in self.families:\n                self.families[machine.family] = machine.group\n\n        self.loc_2_index = {k:e for e, k in enumerate(sorted(set([m.loc for m in self.instance.machines])))}\n        self.group_2_index = {k:e for e, k in enumerate(sorted(set(self.groups.keys())))}\n\n        self.instance.next_decision_point()\n        self.eid = np.random.randint(999_999_999)\n        machines = self.preprocess()\n        self.get_state(machines)\n        if len(self.conflicting_machines) > 0:\n            for family in self.conflicting_machines:\n                return self.state_tensor(family)\n        else:\n            return self.step(None)\n\n    @property\n    def get_machines_t(self):\n        \"\"\"usable machines at time t\"\"\"\n        machines = list(self.instance.usable_machines)\n        families = [m.family for m in machines]\n        machines = {family: {\"counts\": families.count(family), \"machines\":[machine for machine in machines if machine.family == family]} for family in families}\n\n        for family in machines:\n            step_names = set([lot.actual_step.step_name for lot in machines[family][\"machines\"][0].waiting_lots])\n            machines[family][\"lots_groups\"] = {step_name: [lot for lot in machines[family][\"machines\"][0].waiting_lots if lot.actual_step.step_name == step_name] for step_name in step_names}\n        \n        for family in machines:\n            if machines[family][\"counts\"] >= len(machines[family][\"lots_groups\"]):\n                machines[family][\"conflicting\"] = False\n            else:\n                machines[family][\"conflicting\"] = True\n        return machines\n    \n    def dispatch_non_conflicting(self, machines):\n        for family in machines:\n            if not machines[family][\"conflicting\"]:\n                for i, step_name in enumerate(machines[family][\"lots_groups\"]):\n                    self.instance.dispatch(machines[family][\"machines\"][i], machines[family][\"lots_groups\"][step_name])\n\n    def preprocess(self):\n        self.machines = self.get_machines_t\n        self.dispatch_non_conflicting(self.machines)\n        return self.machines\n    \n    def get_state(self, machines):\n        mask = {family:machines[family][\"conflicting\"] for family in machines}\n        conflicting_machines = {family:machines[family] for family in machines if mask[family]}\n        self.conflicting_machines = conflicting_machines\n        return conflicting_machines\n\n    def step(self, action):\n        families = self.conflicting_machines.keys()\n        for family in list(families):\n            if self.conflicting_machines[family][\"counts\"] == 0:\n                self.conflicting_machines.pop(family)\n\n        info = {\"time\": self.instance.current_time, \"done_lots\":[], \"dispatched_lots\": []}\n        new_lots_done = self.instance.done_lots[self.num_lots_done:]\n\n        for lot in new_lots_done:\n            if lot.idx in self.lots_dispatched:\n                self.lots_done.append(lot.idx)\n                info['done_lots'].append(lot)\n                self.throughput += 1\n                lateness_hours = (lot.deadline_at - lot.done_at)/3600\n                self.tardiness += lateness_hours\n                if lateness_hours >= 0:\n                    self.completed_on_time += 1\n\n        self.num_lots_done = len(self.instance.done_lots)\n\n        if len(self.conflicting_machines) == 0:\n            machines = self.preprocess()\n            self.get_state(machines)\n            done = self.instance.next_decision_point()\n            if done or self.instance.current_time > 3600 * 24 * self.days:\n                done = True\n            for family in self.conflicting_machines:\n                return self.state_tensor(family), 0, done, info\n            \n        else:\n            families = self.conflicting_machines.keys()\n            for family in families:\n                if family in self.conflicting_machines.keys():\n                    lot_groups = self.conflicting_machines[family][\"lots_groups\"]\n\n                    for i, step_name in enumerate(lot_groups):\n                        if i == action:\n                            machine = self.conflicting_machines[family][\"machines\"].pop()\n                            lot_group = self.conflicting_machines[family][\"lots_groups\"].pop(step_name)\n                            self.conflicting_machines[family][\"counts\"] -= 1\n                            info[\"dispatched_lots\"].extend([lot.idx for lot in lot_group])\n                            self.lots_dispatched.extend([lot.idx for lot in lot_group])\n                            self.instance.dispatch(machine, lot_group)\n                            break\n                    break\n\n            for family in self.conflicting_machines:\n                if self.conflicting_machines[family][\"counts\"] > 0:\n                    return self.state_tensor(family), 0, False, info\n                \n        return None, 0, False, info\n                    \n    def state_tensor(self, family):\n        def foo(**kwargs):\n            return np.array(list(kwargs.values()))\n            \n        machine_features = foo(num_units=len(self.conflicting_machines[family][\"machines\"]),\n                               group_idx=self.group_2_index[self.families[family]],\n                               num_machine_families_in_group=len(self.groups[self.families[family]]),\n                               load_time_hr=np.mean([m.load_time for m in self.conflicting_machines[family][\"machines\"]])/3600,\n                               unload_time_hr=np.mean([m.unload_time for m in self.conflicting_machines[family][\"machines\"]])/3600,\n                               loc_idx=self.loc_2_index[self.conflicting_machines[family][\"machines\"][0].loc],\n                               num_waiting_lots=len(self.conflicting_machines[family][\"machines\"][0].waiting_lots),\n                               utilized_time=np.mean([m.utilized_time for m in self.conflicting_machines[family][\"machines\"]]),\n                               setuped_time=np.mean([m.setuped_time for m in self.conflicting_machines[family][\"machines\"]]),\n                               pmed_time=np.mean([m.pmed_time for m in self.conflicting_machines[family][\"machines\"]]),\n                               bred_time=np.mean([m.bred_time for m in self.conflicting_machines[family][\"machines\"]]),\n                               min_runs_left_max=np.max([self.none2zero(m.min_runs_left) for m in self.conflicting_machines[family][\"machines\"]]),\n                               min_runs_left_min=np.min([self.none2zero(m.min_runs_left) for m in self.conflicting_machines[family][\"machines\"]]))\n\n        lot_groups_features = []\n        for step_name in self.conflicting_machines[family][\"lots_groups\"]:\n            lot_group: list = self.conflicting_machines[family][\"lots_groups\"][step_name]\n            lot_group_features = [\n                len(lot_group),\n                np.mean([(lot.deadline_at - self.instance.current_time)/3600 for lot in lot_group]),\n                np.max([(lot.deadline_at - self.instance.current_time)/3600 for lot in lot_group]),\n                np.mean([(lot.relative_deadline)/3600 for lot in lot_group]),\n                np.max([(lot.relative_deadline)/3600 for lot in lot_group]),\n                np.mean([(self.instance.current_time - lot.free_since)/3600 for lot in lot_group]),\n                np.max([(self.instance.current_time - lot.free_since)/3600 for lot in lot_group]),\n                np.mean([len(lot.remaining_steps) for lot in lot_group]),\n                np.max([len(lot.remaining_steps) for lot in lot_group]),\n                np.mean([lot.cr(self.instance.current_time) for lot in lot_group]),\n                np.max([lot.cr(self.instance.current_time) for lot in lot_group]),\n                np.mean([lot.priority for lot in lot_group]),\n                np.max([lot.priority for lot in lot_group]),\n                lot_group[0].actual_step.processing_time.avg(),\n                lot_group[0].actual_step.batch_max,\n                lot_group[0].actual_step.batch_min,\n                0 if lot_group[0].actual_step.setup_needed == '' or lot_group[0].actual_step.setup_needed == self.conflicting_machines[family][\"machines\"][0].current_setup else 1\n            ]\n            lot_groups_features.append(np.concatenate([machine_features, np.array(lot_group_features)]))\n\n        return np.stack(lot_groups_features, axis=0).astype(\"float32\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"49fd9ac9-4854-4e5e-a862-4c4c8845c57c","cell_type":"markdown","source":"# Defining the Models using Pytorch","metadata":{}},{"id":"82bb5567","cell_type":"code","source":"class LearnablePositionalEncoding(nn.Module):\n    def __init__(self, seq_len, d_model):\n        super().__init__()\n        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n\n    def forward(self, x):\n        return x + self.pos_embedding[:, :x.shape[1], :]\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, seq_len, input_dim, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.pos_encoder = LearnablePositionalEncoding(seq_len, input_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_dim,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x): \n        x = self.pos_encoder(x)\n        x = self.transformer_encoder(x)\n        return x #N,L,E\n\nclass EMB(nn.Module):\n    def __init__(self, embed_size, hdim, drp=0.1):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_size, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, embed_size),\n        )\n    def forward(self, x):\n        #N,L,E -> N,L,E\n        x = self.mlp(x)\n        return x\n\nclass Actor(nn.Module):\n    def __init__(self, embed_size, hdim, drp=0.1):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_size, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, 1),\n        )\n    def forward(self, x):\n        #N,L,E -> N,L,1\n        x = self.mlp(x)\n        return x.squeeze(-1)\n    \nclass critic(nn.Module):\n    def __init__(self, embed_size, hdim, drp=0.1):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_size, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, hdim),\n            nn.Dropout(drp),\n            nn.GELU(),\n            nn.Linear(hdim, 1),\n        )\n\n    def forward(self, x):\n        #N,L,E -> N,1\n        x = self.mlp(x.mean(1))\n        return x.squeeze(-1)\n\nclass Model(nn.Module):\n    def __init__(self, input_dim, embed_size, seq_len, num_enc, num_heads, hdim, drp=0.1):\n        super().__init__()\n\n        self.embedding = nn.Linear(input_dim, embed_size)\n        self.fe = EMB(embed_size, hdim, drp)\n        self.feature_extractor = FeatureExtractor(seq_len=seq_len, \n                                                  input_dim=embed_size, \n                                                  num_layers=num_enc, \n                                                  nhead=num_heads, \n                                                  dim_feedforward=hdim, \n                                                  dropout=drp)\n        self.actor = Actor(embed_size, hdim, drp)\n        self.critic = critic(embed_size, hdim, drp)\n    \n    def forward(self, x):\n        # x: (N, L, E)\n        unsqueezed = False\n        if x.ndim == 2:\n            x = x.unsqueeze(0)\n            unsqueezed = True\n            \n        x = self.embedding(x)\n        x_a = self.fe(x)\n        x_c = self.feature_extractor(x_a)  # (N, L, E)\n        logits, values = self.actor(x_c), self.critic(x_c)\n\n        if unsqueezed:\n            logits = logits.squeeze(0)\n            values = values.squeeze(0)\n\n        return logits, values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"aefd17d4-0484-41a0-94a7-dad21ce0f418","cell_type":"markdown","source":"# Logic for Rollouts and Reward Retrospection","metadata":{}},{"id":"85978e0a","cell_type":"code","source":"def collect_rollout(first_obs, env, model, *args, rollout_len=2048, is_fifo=False, is_testing=False):\n    obs_buf, action_buf, reward_buf, done_buf, logp_buf, value_buf, info_buf, last_max_checked = args\n    obs = first_obs\n\n    counter = 0\n    pbar = tqdm(total=rollout_len)\n    while counter <= rollout_len:\n        store = False\n        if obs is not None:\n            if is_fifo:\n                action = 0\n                logits =  values = dist = None\n            else:\n                with torch.no_grad():\n                    logits, value = model(torch.from_numpy(obs))\n                    \n                probs = F.softmax(logits, dim=0)\n                dist = Categorical(probs)\n                if is_testing:\n                    action = np.argmax(logits.cpu().numpy().squeeze())\n                else:\n                    action = dist.sample()\n                \n            store = True\n        next_obs, _, done, info = env.step(action if isinstance(action, int) else action.item())\n\n        if store:\n            action_buf.append(action)\n            if is_fifo:\n                logp_buf.append(None)\n                value_buf.append(None)\n            else:\n                logp_buf.append(dist.log_prob(action))\n                value_buf.append(value.squeeze(-1))\n            reward_buf.append(torch.tensor(0, dtype=torch.float32))\n        else:\n            action_buf.append(None)\n            logp_buf.append(None)\n            value_buf.append(None)\n            reward_buf.append(None)\n\n        obs_buf.append(obs)\n        done_buf.append(done)\n        info_buf.append(info)\n            \n        counter += len(info[\"done_lots\"])\n        pbar.update(len(info[\"done_lots\"]))\n        \n        obs = next_obs\n        if done:\n            obs = None\n            break\n    pbar.close()\n\n    if (not is_fifo) and (not is_testing):\n        used_indices = []\n        num_done_lots_for_j = {}\n        for i in tqdm(range(last_max_checked, len(info_buf))):\n            for lot in info_buf[i][\"done_lots\"]:\n                for j in range(i):\n                    if lot.idx in info_buf[j][\"dispatched_lots\"]:\n                        if (lot.deadline_at - lot.done_at) < 0:\n                            reward_buf[j] += (lot.deadline_at - lot.done_at)/3600_00\n                        else:\n                            reward_buf[j] += 1.0\n                        if j not in num_done_lots_for_j:\n                            num_done_lots_for_j[j] = 0\n                        num_done_lots_for_j[j] += 1\n                        used_indices.append(j)\n                        info_buf[j][\"dispatched_lots\"].remove(lot.idx)\n                    \n    \n    last_obs = obs\n    return obs_buf, action_buf, reward_buf, done_buf, logp_buf, value_buf, info_buf, set(used_indices), last_obs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"daf57537-59e8-4c4e-9dcb-87bfb3e93bb9","cell_type":"markdown","source":"# Custom PPO to take in account the complex data structure involved","metadata":{}},{"id":"3831586a","cell_type":"code","source":"def ppo_update(model, optimizer, obs_buf, action_buf, reward_buf, done_buf, logp_buf, value_buf,\n               gamma=0.95, lam=0.95, clip_ratio=0.2, epochs=1, batch_size=32):\n\n    returns = []\n    advs = []\n    gae = 0\n    last_value = 0\n\n    ploss, vloss = [], []\n    for t in reversed(range(len(reward_buf))):\n        mask = 1.0 - float(done_buf[t])\n        delta = reward_buf[t] + gamma * last_value * mask - value_buf[t]\n        gae = delta + gamma * lam * mask * gae\n        advs.insert(0, gae)\n        last_value = value_buf[t]\n        returns.insert(0, gae + value_buf[t])\n\n    advs = torch.tensor(advs, dtype=torch.float32, requires_grad=False)\n    returns = torch.tensor(returns, dtype=torch.float32, requires_grad=False)\n\n    for _ in tqdm(range(epochs)):\n        for i in range(0, len(obs_buf), batch_size):\n            var = [model(torch.from_numpy(i)) for i in obs_buf[i:i+batch_size]]\n            logits, new_values = [i[0] for i in var], torch.tensor([i[1] for i in var])\n            dists = [Categorical(logits=l) for l in logits]\n\n            act_batch = action_buf[i:i+batch_size]\n            old_logp_batch = logp_buf[i:i+batch_size]\n\n            new_logp = []\n            for g in range(len(act_batch)):\n                dist = dists[g]\n                action = act_batch[g]\n                log_prob = dist.log_prob(action)\n                new_logp.append(log_prob)\n\n            ratio = [torch.exp(new_logp_i - old_logp_batch_i) for new_logp_i, old_logp_batch_i in zip(new_logp, old_logp_batch)]\n            adv_batch = advs[i:i+batch_size]\n            ret_batch = returns[i:i+batch_size]\n\n            surr1 = [r*a for r, a in zip(ratio, adv_batch)]\n            surr2 = [torch.clamp(r, 1.0-clip_ratio, 1.0+clip_ratio) * a for r, a in zip(ratio, adv_batch)]\n            policy_loss = -sum([min(s1, s2) for s1, s2 in zip(surr1, surr2)])/len(surr1)\n\n            value_loss = F.mse_loss(new_values.squeeze(-1), ret_batch)\n            loss = policy_loss + 0.25 * value_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            ploss.append(policy_loss.item())\n            vloss.append(value_loss.item())\n    return np.mean(ploss), np.mean(vloss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"478ec071","cell_type":"code","source":"env = SCFabEnv(days=90, \n               dataset=\"SMT2020_HVLM\", \n               dispatcher=\"fifo\", \n               seed=42)\n\nfirst_obs = env.reset()\n\nmodel_path = \"model.pth\"\nmodel = Model(30, 128, 50, 4, 4, 256, 0.1)\nif os.path.isfile(model_path):\n    print(\"Loaded\")\n    state_dict = torch.load(model_path, weights_only=True)\n    model.load_state_dict(state_dict)\n    \noptimizer = Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d184c8cc-9602-4f5a-a873-7eacdb75d586","cell_type":"markdown","source":"# Training Loop","metadata":{}},{"id":"49b364e8","cell_type":"code","source":"if True:\n    import time\n    start_time = time.time()\n    \n    (obs_buf, \n     action_buf, \n     reward_buf, \n     done_buf, \n     logp_buf, \n     value_buf, \n     info_buf) = [], [], [], [], [], [], []\n    \n    logger = Logger(\"logs\")\n    \n    for run in range(10000):\n        if any(done_buf):\n            print(\"Restarting...\")\n            first_obs = env.reset()\n            (obs_buf, \n             action_buf, \n             reward_buf, \n             done_buf, \n             logp_buf, \n             value_buf, \n             info_buf) = [], [], [], [], [], [], []\n    \n        last_max_checked = len(info_buf)\n            \n        print(\"Collecting Rollouts...\")\n        (obs_buf, \n        action_buf, \n        reward_buf, \n        done_buf, \n        logp_buf, \n        value_buf, \n        info_buf, \n        used_indices,\n        last_obs) = collect_rollout(first_obs, \n                                    env, \n                                    model, \n                                    obs_buf, \n                                    action_buf, \n                                    reward_buf, \n                                    done_buf, \n                                    logp_buf, \n                                    value_buf, \n                                    info_buf,\n                                    last_max_checked,\n                                    rollout_len=500)\n    \n        def filter_buf(*args, used_indices):\n            used_indices = sorted(used_indices)\n            rets = [[] for _ in range(len(args))]\n    \n            for i in tqdm(used_indices):\n                for arg, ret in zip(args, rets):\n                    ret.append(arg[i])\n                    \n            return rets\n    \n        print(\"Filtering Data...\")\n        (obs_buf_ij, \n        action_buf_ij, \n        reward_buf_ij, \n        done_buf_ij, \n        logp_buf_ij, \n        value_buf_ij, \n        info_buf_ij) = filter_buf(obs_buf, \n                                action_buf, \n                                reward_buf, \n                                done_buf, \n                                logp_buf, \n                                value_buf, \n                                info_buf,\n                                used_indices=used_indices)\n    \n        print(\"Applying PPO...\")\n        pl, vl = ppo_update(model, \n                            optimizer, \n                            obs_buf_ij, \n                            action_buf_ij, \n                            reward_buf_ij, \n                            done_buf_ij, \n                            logp_buf_ij, \n                            value_buf_ij,\n                            gamma=0.0, \n                            lam=0.95, \n                            clip_ratio=0.2, \n                            epochs=2,\n                            batch_size=50)\n        \n        logger.add_to_pool(eid=env.eid,\n                           reward=np.mean(reward_buf_ij),\n                           throughput=env.throughput,\n                           tardiness=env.tardiness,\n                           policy_loss=pl,\n                           value_loss=vl)\n        logger.commit()\n        torch.save(model.state_dict(), model_path)\n        print(f\"reward: {np.mean(reward_buf_ij):.3f}, throughput: {env.throughput}, tardiness: {env.tardiness}, PL: {pl:.5f}, VL: {vl:.5f}, time passed: {(time.time() - start_time)/60:.4f}\")\n        env.reset(hard=False)\n        print(\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a66e957e-bb92-4d05-b0a9-de341388ee3e","cell_type":"markdown","source":"# Rerunning the simulation using the trained model","metadata":{}},{"id":"76e22d2e-44a2-4bdc-b4d5-737ad5c6fcc8","cell_type":"code","source":"%%time\n\nmodel_path = \"model.pth\"\nmodel = Model(30, 128, 50, 4, 4, 256, 0.1)\nif os.path.isfile(model_path):\n    print(\"Loaded\")\n    state_dict = torch.load(model_path, weights_only=True)\n    model.load_state_dict(state_dict)\nmodel.eval()\n\nenv = SCFabEnv(days=365, \n               dataset=\"SMT2020_HVLM\", \n               dispatcher=\"fifo\", \n               seed=42)\nfirst_obs = env.reset()\n\n\n(obs_buf, \n action_buf, \n reward_buf, \n done_buf, \n logp_buf, \n value_buf, \n info_buf) = [], [], [], [], [], [], []\n\nlast_max_checked = len(info_buf)\n\n(obs_buf, \naction_buf, \nreward_buf, \ndone_buf, \nlogp_buf, \nvalue_buf, \ninfo_buf, \nused_indices,\nlast_obs) = collect_rollout(first_obs, \n                            env, \n                            model, \n                            obs_buf, \n                            action_buf, \n                            reward_buf, \n                            done_buf, \n                            logp_buf, \n                            value_buf, \n                            info_buf,\n                            last_max_checked,\n                            rollout_len=10000000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bb58e416-626f-469f-b7dc-f38ed4d49a4c","cell_type":"code","source":"import copy\ntarined_env = copy.deepcopy(env)\nenv.throughput, env.tardiness/3600/24, env.completed_on_time, env.completed_on_time/env.throughput","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2fdf8080-b640-4f6f-9724-d3e0b7bc4395","cell_type":"markdown","source":"# Rerunning the simulation using the FIFO","metadata":{}},{"id":"bac0b40e-0e53-4dd2-80e8-3248ea9e7471","cell_type":"code","source":"%%time\n\nmodel_path = \"model.pth\"\nmodel = Model(30, 128, 50, 4, 4, 256, 0.1)\nif os.path.isfile(model_path):\n    print(\"Loaded\")\n    state_dict = torch.load(model_path, weights_only=True)\n    model.load_state_dict(state_dict)\nmodel.eval()\n\nenv = SCFabEnv(days=365, \n               dataset=\"SMT2020_HVLM\", \n               dispatcher=\"fifo\", \n               seed=42)\nfirst_obs = env.reset()\n\n\n(obs_buf, \n action_buf, \n reward_buf, \n done_buf, \n logp_buf, \n value_buf, \n info_buf) = [], [], [], [], [], [], []\n\nlast_max_checked = len(info_buf)\n\n(obs_buf, \naction_buf, \nreward_buf, \ndone_buf, \nlogp_buf, \nvalue_buf, \ninfo_buf, \nused_indices,\nlast_obs) = collect_rollout(first_obs, \n                            env, \n                            model, \n                            obs_buf, \n                            action_buf, \n                            reward_buf, \n                            done_buf, \n                            logp_buf, \n                            value_buf, \n                            info_buf,\n                            last_max_checked,\n                            rollout_len=10000000,\n                            is_fifo=True\n                           )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2672750e-a84c-407b-a2b1-d17668e986da","cell_type":"code","source":"env.throughput, env.tardiness/3600/24, env.completed_on_time, env.completed_on_time/env.throughput","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b11d2c49-0ab5-4e80-b915-dc503ffbff2b","cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"whitegrid\")\n\nf = pd.read_csv(\"/kaggle/working/training_logs.csv\")\nwindow = 5\n\nfor col in f.columns:\n    if \"eid\" in col or \"named\" in col:\n        continue\n\n    series = f[col]\n    rolling_mean = series.rolling(window=window, center=True).mean()\n    rolling_std = series.rolling(window=window, center=True).std()\n    lower = rolling_mean - rolling_std\n    upper = rolling_mean + rolling_std\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(rolling_mean, label=f\"{col} (Rolling Mean)\", color=\"navy\", linewidth=2)\n    plt.fill_between(f.index, lower, upper, color=\"navy\", alpha=0.2)\n\n    plt.title(col, fontsize=14)\n    plt.xlabel(\"Updates\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bcb16927-f0b3-445c-afb7-98d708722020","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}